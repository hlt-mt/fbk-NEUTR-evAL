# Multilingual Gender-Neutral Text and Translation Evaluation

This repository contains the code to perform LLM-based gender-neutrality evaluation in German, Italian, and Spanish texts and gender-neutral translation evaluation from English into those languages and Greek. \
This approach can be used in conjunction with [mGeNTE](https://huggingface.co/datasets/FBK-MT/mGeNTE) to benchmark gender-neutral reformulation and translation in multiple languages.

## Evaluation formats

There are four formats available for the evaluation, as described in [the paper](https://arxiv.org/abs/2504.11934):

* **MONO-L**: evaluates gender-neutrality in German, Italian, or Spanish texts simply generating the label `GENDERED` if the text expresses the gender of human beings mentioned, or the label `NEUTRAL` if the text is completely gender-neutral.
* **MONO-P+L**: evaluates gender-neutrality by first generating phrase-level annotations and then either the `GENDERED` or `NEUTRAL` label with the same rationale as MONO-L.
* **CROSS-L**: assesses gender-neutral translation from English into German, Italian, or Spanish with either the label `NEUTRAL` if the target text is completely gender-neutral, `CORRECTLY GENDERED` if the gender expression in the target matches gender information available in the source, or `WRONGLY GENDERED` if the gender expressions in the target do not match gender information in the source or the target adds gender information while the source lacks it.
* **CROSS-P+L**: assesses gender-neutral translation by generating phrase-level annotations first and then one sentence-level label: `NEUTRAL`, `CORRECTLY GENDERED`, or `WRONGLY GENDERED`, with the same rationale as CROSS-L.

By default, the `CROSS-P+L` evaluation format will be applied. To evaluate with other formats see the section [How to run](#how-to-run).

The evaluations are saved progressively in [*JSON Lines* format](https://jsonlines.org/).

## How to run

Ensure you have installed this repository following the instructions in the [README](../README.md).

Then, you can run the evaluation either using local models compatible with the `transformers` library (e.g., models downloaded from Hugging Face) or via OpenAI's API.

### Preparation

If you want to evaluate **gender-neutral translation from English into German, Greek, Italian, or Spanish**, organize the data in a TSV file containing the following columns:
* `src`: this column must contain the English source sentences
* `tgt`: this column must contain the target language translations

If you want to evaluate **gender neutrality in German, Italian, or Spanish texts**, the TSV file just needs to contain the `tgt` column.

### Using local models

You can run the local LLM-based evaluation script with the following command:
```bash
llm_eval -i $INPUT \
          -l $LANGUAGE_CODE \
          -o $OUTPUT_PATH 
```

**Parameters:**
- `$INPUT` is the path to the input TSV file.
- `$LANGUAGE_CODE` identifies the specific langauge to be evaluated, either `de`, `el`, `es`, or `it` for German, Greek, Spanish, or Italian respectively.
- `$OUTPUT_PATH` is the path and name for the output *JSON Lines* file.

**Optional parameters:**
- `-p` or `--prompt`: Indicate which evaluation format to use among `mono-l`, `mono-p_l`, `cross-l`, and `cross-p_l`. Defaults to `cross-p_l`. For Greek, only `cross-p_l` is available.
- `-m` or `--model`: Hugging Face model identifier to use a specific model. Defaults to `Qwen/Qwen2.5-72B-Instruct`.
- `-r` or `--start_index`: Indicate the starting index for the evaluation. Defaults to 0.
- `-v` or `--verbose`: Specify to show all model's outputs to the standard output.

### GPT-based evaluation

To use this method, you will need an OpenAI API key saved in a plain text file.

Note that since OpenAI's API for structured generation no longer support the `"minItems"` and `"maxItems"` constraints in JSON schemas, we created dedicated schemas for GPT-based evaluation that do not include those constraints.
While this allows for GNT evaluation with GPT models—which were proven to be very accurate—we cannot enforce the `phrases` list of annotations generated by the model not to be empty.

You can run the GPT-based evaluation script with the following command:
```bash
gpt_eval -i $INPUT \
          -l $LANGUAGE_CODE \
          -o $OUTPUT_PATH \
          -k $OPENAI_API_KEY_FILE
```

**Parameters:**
- `$INPUT` is the path to the input TSV file.
- `$LANGUAGE_CODE` identifies the specific langauge to be evaluated, either `de`, `el`, `es`, or `it` for German, Greek, Spanish, or Italian respectively.
- `$OUTPUT_PATH` is the path and name for the output *JSON Lines* file.
- `$OPENAI_API_KEY_FILE` is the path to a plain text file containing an OpenAI API key.

**Optional parameters:**
- `-p` or `--prompt`: Indicate which evaluation format to use among `mono-l`, `mono-p_l`, `cross-l`, and `cross-p_l`. Defaults to `cross-p_l`. For Greek, only `cross-p_l` is available.
- `-m` or `--model`: OpenAI model identifier to use a specific endpoint. Make sure that your endpoint supports structured generation. Defaults to `gpt-4o-2024-08-06`.
- `--org_file`: Path to a plain text file containing an OpenAI organization ID.
- `-r` or `--start_index`: Indicate the starting index for the evaluation. Defaults to 0.
- `-v` or `--verbose`: Specify to show all model's outputs to the standard output.


## How to cite

If you use the code or data included in this repository, please cite the paper [An LLM-as-a-judge Approach for Scalable Gender-Neutral Translation Evaluation](https://arxiv.org/abs/2504.11934):


```
@misc{piergentili2025llmasajudgeapproachscalablegenderneutral,
      title={An LLM-as-a-judge Approach for Scalable Gender-Neutral Translation Evaluation}, 
      author={Andrea Piergentili and Beatrice Savoldi and Matteo Negri and Luisa Bentivogli},
      year={2025},
      eprint={2504.11934},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2504.11934}, 
}
```